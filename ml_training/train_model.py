#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
AIè¯Šæ–­æ¨¡å‹è®­ç»ƒè„šæœ¬
æ”¯æŒé€»è¾‘å›å½’ã€éšæœºæ£®æ—ã€XGBoostç­‰å¤šç§æ¨¡å‹
ç›®æ ‡ï¼šå‡†ç¡®ç‡è¾¾åˆ°85%+
"""

import os
import json
import pickle
import numpy as np
import pandas as pd
from datetime import datetime
from sklearn.model_selection import train_test_split, GridSearchCV, cross_val_score
from sklearn.preprocessing import StandardScaler, LabelEncoder
from sklearn.linear_model import LogisticRegression
from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier
from sklearn.metrics import (
    accuracy_score, precision_score, recall_score, f1_score,
    roc_auc_score, confusion_matrix, classification_report
)
import pymysql
import warnings
warnings.filterwarnings('ignore')

# ================== é…ç½® ==================
DB_CONFIG = {
    'host': 'localhost',
    'port': 3306,
    'user': 'root',
    'password': 'your_password',  # ä¿®æ”¹ä¸ºå®é™…å¯†ç 
    'database': 'sm_medical',
    'charset': 'utf8mb4'
}

MODEL_DIR = './models'
LOG_DIR = './logs'
os.makedirs(MODEL_DIR, exist_ok=True)
os.makedirs(LOG_DIR, exist_ok=True)

# ç–¾ç—…åˆ†ç±»æ˜ å°„
DISEASE_MAPPING = {
    'é«˜è¡€å‹': ['é«˜è¡€å‹1çº§', 'é«˜è¡€å‹2çº§', 'é«˜è¡€å‹3çº§', 'è½»åº¦é«˜è¡€å‹', 'ä¸­åº¦é«˜è¡€å‹', 'é‡åº¦é«˜è¡€å‹'],
    'ä½è¡€å‹': ['ä½è¡€å‹', 'è¡€å‹åä½'],
    'å¿ƒå¾‹å¤±å¸¸': ['å¿ƒåŠ¨è¿‡é€Ÿ', 'å¿ƒåŠ¨è¿‡ç¼“', 'å¿ƒç‡è¿‡å¿«', 'å¿ƒç‡è¿‡æ…¢'],
    'ç³–å°¿ç—…': ['ç³–å°¿ç—…', 'é«˜è¡€ç³–', 'ç©ºè…¹è¡€ç³–å¼‚å¸¸'],
    'ä½è¡€ç³–': ['ä½è¡€ç³–', 'è¡€ç³–åä½'],
    'æ„ŸæŸ“': ['å‘çƒ­', 'é«˜çƒ­', 'æ„Ÿå†’', 'ä¸Šå‘¼å¸é“æ„ŸæŸ“'],
    'æ¶ˆåŒ–ç³»ç»Ÿ': ['æ€¥æ€§èƒƒç‚', 'èƒƒç‚', 'è…¹ç—›', 'è…¹æ³»'],
    'ç¥ç»ç³»ç»Ÿ': ['åå¤´ç—›', 'å¤´ç—›', 'å¤´æ™•']
}


class MedicalAITrainer:
    """åŒ»ç–—AIæ¨¡å‹è®­ç»ƒå™¨"""
    
    def __init__(self, db_config):
        self.db_config = db_config
        self.conn = None
        self.scaler = StandardScaler()
        self.label_encoder = LabelEncoder()
        self.feature_names = []
        self.model = None
        self.model_type = None
        
    def connect_db(self):
        """è¿æ¥æ•°æ®åº“"""
        try:
            self.conn = pymysql.connect(**self.db_config)
            print("âœ… æ•°æ®åº“è¿æ¥æˆåŠŸ")
            return True
        except Exception as e:
            print(f"âŒ æ•°æ®åº“è¿æ¥å¤±è´¥: {e}")
            return False
    
    def load_training_data(self, min_samples_per_class=10):
        """ä»æ•°æ®åº“åŠ è½½è®­ç»ƒæ•°æ®"""
        if not self.conn:
            if not self.connect_db():
                return None, None
        
        # æŸ¥è¯¢é«˜è´¨é‡è®­ç»ƒæ•°æ®
        query = """
        SELECT 
            age, gender, systolic_bp, diastolic_bp, heart_rate,
            temperature, blood_sugar, bmi,
            symptom_keywords, symptom_severity,
            has_hypertension, has_diabetes, has_heart_disease,
            smoking_status, drinking_status,
            doctor_diagnosis, diagnosis_icd10
        FROM SM_ai_training_data
        WHERE is_verified = 1 
          AND data_quality IN (1, 2)
          AND doctor_diagnosis IS NOT NULL
          AND doctor_diagnosis != ''
        ORDER BY created_at DESC
        """
        
        try:
            df = pd.read_sql(query, self.conn)
            print(f"ğŸ“Š åŠ è½½æ•°æ®: {len(df)} æ¡è®°å½•")
            
            if len(df) < 50:
                print(f"âš ï¸  æ•°æ®é‡ä¸è¶³({len(df)}æ¡)ï¼Œå»ºè®®è‡³å°‘50æ¡ï¼Œå½“å‰ä½¿ç”¨è§„åˆ™å¼•æ“")
                return None, None
            
            # æ•°æ®é¢„å¤„ç†
            df = self._preprocess_data(df)
            
            # è¿‡æ»¤å°æ ·æœ¬ç±»åˆ«
            disease_counts = df['disease_category'].value_counts()
            valid_diseases = disease_counts[disease_counts >= min_samples_per_class].index
            df = df[df['disease_category'].isin(valid_diseases)]
            
            print(f"âœ… æœ‰æ•ˆæ•°æ®: {len(df)} æ¡")
            print(f"ğŸ“‹ ç–¾ç—…ç±»åˆ«: {len(valid_diseases)} ä¸ª")
            print(f"ğŸ“ˆ ç±»åˆ«åˆ†å¸ƒ:\n{df['disease_category'].value_counts()}")
            
            # åˆ†ç¦»ç‰¹å¾å’Œæ ‡ç­¾
            X = df.drop(['doctor_diagnosis', 'diagnosis_icd10', 'disease_category'], axis=1)
            y = df['disease_category']
            
            self.feature_names = X.columns.tolist()
            
            return X, y
            
        except Exception as e:
            print(f"âŒ æ•°æ®åŠ è½½å¤±è´¥: {e}")
            return None, None
    
    def _preprocess_data(self, df):
        """æ•°æ®é¢„å¤„ç†"""
        # 1. å¤„ç†ç¼ºå¤±å€¼
        numeric_cols = ['age', 'systolic_bp', 'diastolic_bp', 'heart_rate', 
                       'temperature', 'blood_sugar', 'bmi', 'symptom_severity']
        for col in numeric_cols:
            if col in df.columns:
                df[col] = df[col].fillna(df[col].median())
        
        # 2. å¤„ç†ç—‡çŠ¶å…³é”®è¯ï¼ˆæ–‡æœ¬ç‰¹å¾ï¼‰
        if 'symptom_keywords' in df.columns:
            # æå–å…³é”®ç—‡çŠ¶ç‰¹å¾
            df['has_headache'] = df['symptom_keywords'].apply(
                lambda x: 1 if isinstance(x, str) and ('å¤´ç—›' in x or 'å¤´æ™•' in x) else 0
            )
            df['has_fever'] = df['symptom_keywords'].apply(
                lambda x: 1 if isinstance(x, str) and ('å‘çƒ­' in x or 'å‘çƒ§' in x) else 0
            )
            df['has_chest_pain'] = df['symptom_keywords'].apply(
                lambda x: 1 if isinstance(x, str) and ('èƒ¸ç—›' in x or 'èƒ¸é—·' in x) else 0
            )
            df['has_cough'] = df['symptom_keywords'].apply(
                lambda x: 1 if isinstance(x, str) and 'å’³å—½' in x else 0
            )
            df['has_abdominal_pain'] = df['symptom_keywords'].apply(
                lambda x: 1 if isinstance(x, str) and 'è…¹ç—›' in x else 0
            )
            df = df.drop('symptom_keywords', axis=1)
        
        # 3. æ ‡å‡†åŒ–ç–¾ç—…è¯Šæ–­ï¼ˆæ˜ å°„åˆ°å¤§ç±»ï¼‰
        df['disease_category'] = df['doctor_diagnosis'].apply(self._map_disease_category)
        
        # 4. å¤„ç†å¸ƒå°”å€¼
        bool_cols = ['has_hypertension', 'has_diabetes', 'has_heart_disease']
        for col in bool_cols:
            if col in df.columns:
                df[col] = df[col].fillna(0).astype(int)
        
        # 5. å¤„ç†åˆ†ç±»å˜é‡
        if 'gender' in df.columns:
            df['gender'] = df['gender'].fillna(0).astype(int)
        if 'smoking_status' in df.columns:
            df['smoking_status'] = df['smoking_status'].fillna(0).astype(int)
        if 'drinking_status' in df.columns:
            df['drinking_status'] = df['drinking_status'].fillna(0).astype(int)
        if 'symptom_severity' in df.columns:
            df['symptom_severity'] = df['symptom_severity'].fillna(5).astype(int)
        
        # 6. è®¡ç®—æ´¾ç”Ÿç‰¹å¾
        if 'systolic_bp' in df.columns and 'diastolic_bp' in df.columns:
            df['pulse_pressure'] = df['systolic_bp'] - df['diastolic_bp']  # è„‰å‹å·®
            df['mean_arterial_pressure'] = (df['systolic_bp'] + 2 * df['diastolic_bp']) / 3  # å¹³å‡åŠ¨è„‰å‹
        
        if 'age' in df.columns:
            df['age_group'] = pd.cut(df['age'], bins=[0, 18, 40, 60, 100], 
                                    labels=[0, 1, 2, 3]).astype(int)
        
        return df
    
    def _map_disease_category(self, diagnosis):
        """å°†å…·ä½“ç–¾ç—…æ˜ å°„åˆ°å¤§ç±»"""
        if not isinstance(diagnosis, str):
            return 'å…¶ä»–'
        
        for category, keywords in DISEASE_MAPPING.items():
            for keyword in keywords:
                if keyword in diagnosis:
                    return category
        return 'å…¶ä»–'
    
    def train_logistic_regression(self, X_train, y_train):
        """è®­ç»ƒé€»è¾‘å›å½’æ¨¡å‹"""
        print("\nğŸ”§ è®­ç»ƒé€»è¾‘å›å½’æ¨¡å‹...")
        
        param_grid = {
            'C': [0.001, 0.01, 0.1, 1, 10, 100],
            'penalty': ['l2'],
            'solver': ['lbfgs', 'saga'],
            'max_iter': [1000]
        }
        
        lr = LogisticRegression(random_state=42, multi_class='multinomial')
        grid_search = GridSearchCV(lr, param_grid, cv=5, scoring='accuracy', n_jobs=-1)
        grid_search.fit(X_train, y_train)
        
        print(f"âœ… æœ€ä½³å‚æ•°: {grid_search.best_params_}")
        print(f"âœ… äº¤å‰éªŒè¯å‡†ç¡®ç‡: {grid_search.best_score_:.4f}")
        
        return grid_search.best_estimator_
    
    def train_random_forest(self, X_train, y_train):
        """è®­ç»ƒéšæœºæ£®æ—æ¨¡å‹"""
        print("\nğŸŒ² è®­ç»ƒéšæœºæ£®æ—æ¨¡å‹...")
        
        param_grid = {
            'n_estimators': [50, 100, 200],
            'max_depth': [5, 10, 15, None],
            'min_samples_split': [2, 5, 10],
            'min_samples_leaf': [1, 2, 4]
        }
        
        rf = RandomForestClassifier(random_state=42, n_jobs=-1)
        grid_search = GridSearchCV(rf, param_grid, cv=5, scoring='accuracy', n_jobs=-1)
        grid_search.fit(X_train, y_train)
        
        print(f"âœ… æœ€ä½³å‚æ•°: {grid_search.best_params_}")
        print(f"âœ… äº¤å‰éªŒè¯å‡†ç¡®ç‡: {grid_search.best_score_:.4f}")
        
        return grid_search.best_estimator_
    
    def train_gradient_boosting(self, X_train, y_train):
        """è®­ç»ƒæ¢¯åº¦æå‡æ ‘æ¨¡å‹"""
        print("\nğŸš€ è®­ç»ƒæ¢¯åº¦æå‡æ ‘æ¨¡å‹...")
        
        param_grid = {
            'n_estimators': [50, 100, 200],
            'learning_rate': [0.01, 0.05, 0.1],
            'max_depth': [3, 5, 7],
            'min_samples_split': [2, 5],
            'subsample': [0.8, 1.0]
        }
        
        gb = GradientBoostingClassifier(random_state=42)
        grid_search = GridSearchCV(gb, param_grid, cv=5, scoring='accuracy', n_jobs=-1)
        grid_search.fit(X_train, y_train)
        
        print(f"âœ… æœ€ä½³å‚æ•°: {grid_search.best_params_}")
        print(f"âœ… äº¤å‰éªŒè¯å‡†ç¡®ç‡: {grid_search.best_score_:.4f}")
        
        return grid_search.best_estimator_
    
    def evaluate_model(self, model, X_test, y_test):
        """è¯„ä¼°æ¨¡å‹æ€§èƒ½"""
        print("\nğŸ“Š æ¨¡å‹è¯„ä¼°...")
        
        y_pred = model.predict(X_test)
        y_pred_proba = model.predict_proba(X_test) if hasattr(model, 'predict_proba') else None
        
        # åŸºæœ¬æŒ‡æ ‡
        accuracy = accuracy_score(y_test, y_pred)
        precision = precision_score(y_test, y_pred, average='weighted', zero_division=0)
        recall = recall_score(y_test, y_pred, average='weighted', zero_division=0)
        f1 = f1_score(y_test, y_pred, average='weighted', zero_division=0)
        
        # æ··æ·†çŸ©é˜µ
        cm = confusion_matrix(y_test, y_pred)
        
        # AUC (å¤šåˆ†ç±»)
        auc = 0.0
        if y_pred_proba is not None and len(np.unique(y_test)) > 1:
            try:
                auc = roc_auc_score(y_test, y_pred_proba, multi_class='ovr', average='weighted')
            except:
                pass
        
        print(f"\nâœ… å‡†ç¡®ç‡ (Accuracy): {accuracy:.4f}")
        print(f"âœ… ç²¾ç¡®ç‡ (Precision): {precision:.4f}")
        print(f"âœ… å¬å›ç‡ (Recall): {recall:.4f}")
        print(f"âœ… F1åˆ†æ•°: {f1:.4f}")
        if auc > 0:
            print(f"âœ… AUC: {auc:.4f}")
        
        print(f"\næ··æ·†çŸ©é˜µ:\n{cm}")
        
        # åˆ†ç±»æŠ¥å‘Š
        print(f"\nè¯¦ç»†åˆ†ç±»æŠ¥å‘Š:")
        print(classification_report(y_test, y_pred, zero_division=0))
        
        return {
            'accuracy': accuracy,
            'precision': precision,
            'recall': recall,
            'f1_score': f1,
            'auc_score': auc,
            'confusion_matrix': cm.tolist()
        }
    
    def save_model(self, model, metrics, model_type='random_forest'):
        """ä¿å­˜æ¨¡å‹"""
        timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')
        version = f"v{timestamp}"
        
        # ä¿å­˜æ¨¡å‹æ–‡ä»¶
        model_filename = f"{model_type}_{version}.pkl"
        model_path = os.path.join(MODEL_DIR, model_filename)
        
        model_data = {
            'model': model,
            'scaler': self.scaler,
            'label_encoder': self.label_encoder,
            'feature_names': self.feature_names,
            'model_type': model_type,
            'version': version,
            'metrics': metrics,
            'trained_at': datetime.now().isoformat()
        }
        
        with open(model_path, 'wb') as f:
            pickle.dump(model_data, f)
        
        print(f"\nâœ… æ¨¡å‹å·²ä¿å­˜: {model_path}")
        
        # ä¿å­˜åˆ°æ•°æ®åº“
        self._save_model_to_db(model_filename, metrics, model_type, version)
        
        return model_path
    
    def _save_model_to_db(self, model_filename, metrics, model_type, version):
        """ä¿å­˜æ¨¡å‹ä¿¡æ¯åˆ°æ•°æ®åº“"""
        try:
            cursor = self.conn.cursor()
            
            sql = """
            INSERT INTO SM_ai_model_version (
                model_name, version, model_type,
                accuracy, precision_score, recall_score, f1_score, auc_score,
                confusion_matrix, model_file_path,
                description
            ) VALUES (%s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s)
            """
            
            cursor.execute(sql, (
                'medical_diagnosis_ai',
                version,
                model_type,
                metrics['accuracy'],
                metrics['precision'],
                metrics['recall'],
                metrics['f1_score'],
                metrics['auc_score'],
                json.dumps(metrics['confusion_matrix']),
                model_filename,
                f"è‡ªåŠ¨è®­ç»ƒæ¨¡å‹ - {model_type}"
            ))
            
            self.conn.commit()
            print(f"âœ… æ¨¡å‹ä¿¡æ¯å·²ä¿å­˜åˆ°æ•°æ®åº“")
            
        except Exception as e:
            print(f"âš ï¸  ä¿å­˜æ¨¡å‹ä¿¡æ¯å¤±è´¥: {e}")
    
    def train_all_models(self):
        """è®­ç»ƒå¹¶æ¯”è¾ƒæ‰€æœ‰æ¨¡å‹"""
        print("="*60)
        print("ğŸš€ å¼€å§‹è®­ç»ƒAIè¯Šæ–­æ¨¡å‹")
        print("="*60)
        
        # 1. åŠ è½½æ•°æ®
        X, y = self.load_training_data()
        if X is None or y is None:
            print("âŒ æ•°æ®åŠ è½½å¤±è´¥ï¼Œè®­ç»ƒç»ˆæ­¢")
            return
        
        # 2. æ•°æ®æ ‡å‡†åŒ–
        X_scaled = self.scaler.fit_transform(X)
        y_encoded = self.label_encoder.fit_transform(y)
        
        # 3. åˆ’åˆ†è®­ç»ƒé›†å’Œæµ‹è¯•é›†
        X_train, X_test, y_train, y_test = train_test_split(
            X_scaled, y_encoded, test_size=0.2, random_state=42, stratify=y_encoded
        )
        
        print(f"\nğŸ“Š æ•°æ®åˆ’åˆ†:")
        print(f"  è®­ç»ƒé›†: {len(X_train)} æ¡")
        print(f"  æµ‹è¯•é›†: {len(X_test)} æ¡")
        
        # 4. è®­ç»ƒå¤šä¸ªæ¨¡å‹
        models = {}
        model_metrics = {}
        
        # é€»è¾‘å›å½’
        lr_model = self.train_logistic_regression(X_train, y_train)
        models['logistic_regression'] = lr_model
        model_metrics['logistic_regression'] = self.evaluate_model(lr_model, X_test, y_test)
        
        # éšæœºæ£®æ—
        rf_model = self.train_random_forest(X_train, y_train)
        models['random_forest'] = rf_model
        model_metrics['random_forest'] = self.evaluate_model(rf_model, X_test, y_test)
        
        # æ¢¯åº¦æå‡æ ‘
        gb_model = self.train_gradient_boosting(X_train, y_train)
        models['gradient_boosting'] = gb_model
        model_metrics['gradient_boosting'] = self.evaluate_model(gb_model, X_test, y_test)
        
        # 5. é€‰æ‹©æœ€ä½³æ¨¡å‹
        best_model_type = max(model_metrics, key=lambda k: model_metrics[k]['accuracy'])
        best_model = models[best_model_type]
        best_metrics = model_metrics[best_model_type]
        
        print("\n" + "="*60)
        print(f"ğŸ† æœ€ä½³æ¨¡å‹: {best_model_type}")
        print(f"ğŸ¯ å‡†ç¡®ç‡: {best_metrics['accuracy']:.4f}")
        print("="*60)
        
        # 6. ä¿å­˜æœ€ä½³æ¨¡å‹
        model_path = self.save_model(best_model, best_metrics, best_model_type)
        
        # 7. ç‰¹å¾é‡è¦æ€§åˆ†æï¼ˆå¦‚æœæ”¯æŒï¼‰
        if hasattr(best_model, 'feature_importances_'):
            self._plot_feature_importance(best_model)
        
        print("\nâœ… è®­ç»ƒå®Œæˆï¼")
        
        return best_model, best_metrics
    
    def _plot_feature_importance(self, model):
        """åˆ†æç‰¹å¾é‡è¦æ€§"""
        if not hasattr(model, 'feature_importances_'):
            return
        
        importance = model.feature_importances_
        feature_importance = pd.DataFrame({
            'feature': self.feature_names,
            'importance': importance
        }).sort_values('importance', ascending=False)
        
        print("\nğŸ“Š ç‰¹å¾é‡è¦æ€§ TOP 10:")
        print(feature_importance.head(10).to_string(index=False))
    
    def close(self):
        """å…³é—­æ•°æ®åº“è¿æ¥"""
        if self.conn:
            self.conn.close()
            print("\nâœ… æ•°æ®åº“è¿æ¥å·²å…³é—­")


def main():
    """ä¸»å‡½æ•°"""
    trainer = MedicalAITrainer(DB_CONFIG)
    
    try:
        best_model, metrics = trainer.train_all_models()
        
        if metrics['accuracy'] >= 0.85:
            print(f"\nğŸ‰ æ­å–œï¼æ¨¡å‹å‡†ç¡®ç‡è¾¾åˆ° {metrics['accuracy']:.2%}ï¼Œè¶…è¿‡85%ç›®æ ‡ï¼")
        else:
            print(f"\nâš ï¸  æ¨¡å‹å‡†ç¡®ç‡ {metrics['accuracy']:.2%}ï¼Œæœªè¾¾åˆ°85%ç›®æ ‡")
            print("å»ºè®®ï¼š")
            print("  1. æ”¶é›†æ›´å¤šè®­ç»ƒæ•°æ®ï¼ˆå½“å‰å¯èƒ½ä¸è¶³ï¼‰")
            print("  2. å¢åŠ æ›´å¤šç‰¹å¾å·¥ç¨‹")
            print("  3. è°ƒæ•´æ¨¡å‹è¶…å‚æ•°")
        
    except Exception as e:
        print(f"\nâŒ è®­ç»ƒå¤±è´¥: {e}")
        import traceback
        traceback.print_exc()
    
    finally:
        trainer.close()


if __name__ == '__main__':
    main()
